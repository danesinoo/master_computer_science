{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e466b8c",
   "metadata": {},
   "source": [
    "# Logistic Regression with PyTorch Home Assignment \n",
    "## Christian Igel, 2023\n",
    "\n",
    "In the following, we consider an example where 2D data points are classified using logistic regression and the solution is visualized. \n",
    "\n",
    "First, an implementation in Scikit-Learn is given.\n",
    "Then the same is partly implemented using PyTorch.\n",
    "Your assignment is to fill in the blanks in the PyTorch implementation.\n",
    "\n",
    "## Reference implementation using Scikit-Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7230b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c642f",
   "metadata": {},
   "source": [
    "Generate toy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaf9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4  # Number of classes\n",
    "d = 2  # Inout dimensionality\n",
    "train_n = 50  # Training set size\n",
    "test_n = 25  # Test set size\n",
    "\n",
    "# Generate data with three classes\n",
    "X, y = make_classification(n_samples=test_n + train_n, n_features=d, n_informative=d, n_redundant=0, n_repeated=0, n_classes=m, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=0.75, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_n, random_state=4711)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba2de3d",
   "metadata": {},
   "source": [
    "### Model training and evaluation \n",
    "Train and evaluate model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121350b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression\n",
    "logreg = LogisticRegression(penalty=None, fit_intercept=True, multi_class='multinomial', solver='lbfgs')\n",
    "logreg.fit(X_train, y_train);\n",
    "# Get model parameters\n",
    "ws = logreg.coef_\n",
    "bs = logreg.intercept_\n",
    "# Evaluate model\n",
    "train_score = logreg.score(X_train, y_train)\n",
    "test_score = logreg.score(X_test, y_test)\n",
    "print(\"Training error:\", train_score, \"Test error:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dbde0d",
   "metadata": {},
   "source": [
    "### Model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed19520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors for the three classes\n",
    "colors = ['b', 'r', 'g', 'y']\n",
    "class_colours = [colors[i] for i in y] \n",
    "cmap = matplotlib.colors.ListedColormap(colors[:m])\n",
    "\n",
    "# Compute the plot boundaries\n",
    "xl, xh = np.floor(X[:,0].min() - 0.1), np.ceil(X[:,0].max() + 0.1) \n",
    "yl, yh = np.floor(X[:,1].min() - 0.1), np.ceil(X[:,1].max() + 0.1)\n",
    "plt.xlim(xl, xh)\n",
    "plt.ylim(yl, yh)\n",
    "\n",
    "# Create grid to calculate the decision boundary on\n",
    "res = (xh-xl)/300  # Resulution of the grid\n",
    "xx, yy = np.meshgrid(np.arange(xl, xh, res), np.arange(yl, yh, res))\n",
    "\n",
    "# Make plot\n",
    "plt.figure(1)\n",
    "ax = plt.gca()\n",
    "\n",
    "# Classify each point on the grid\n",
    "Z = logreg.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = np.argmax(Z, axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot points\n",
    "ax.scatter(X[:,0], X[:,1], s=45, c=class_colours, edgecolor=plt.cm.gray(.95), lw=0.5, zorder=100)\n",
    "\n",
    "# Plot classifications\n",
    "cax = ax.matshow(Z, cmap=cmap, origin=\"lower\", extent=[xl, xh, yl, yh], aspect=\"auto\", alpha=.4)       \n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.grid(False)\n",
    "\n",
    "if(m==2):  # Special case 2 classes\n",
    "    print(ws)\n",
    "    b = -bs/ws[0,1]\n",
    "    a = -ws[0,0]/ws[0,1]\n",
    "    x_line = np.arange(xl, xh, res)\n",
    "    y_line = a * x_line + b \n",
    "    ax.plot(x_line, y_line, 'k', lw=1, ls='--')\n",
    "else:\n",
    "    for i in np.arange(m-1):\n",
    "        for j in np.arange(i+1,m):\n",
    "            w = ws[i]-ws[j]\n",
    "            b = bs[i]-bs[j]\n",
    "            b = -b/w[1]\n",
    "            a = -w[0]/w[1]\n",
    "            x_line = np.arange(xl, xh, res)\n",
    "            y_line = a * x_line + b \n",
    "            ax.plot(x_line, y_line, \"k\", lw=1, ls='--')\n",
    "\n",
    "plt.xlim(xl, xh)\n",
    "plt.ylim(yl, yh)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef33a813",
   "metadata": {},
   "source": [
    " ## Pytorch\n",
    " Now the task is to reproduce the code above using logistic regression implemented in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68136f3a",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "Let's define the logistic regression model as a simpl neural network with a single layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a282a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionPytorch(nn.Module):\n",
    "    def __init__(self, d, m):\n",
    "        super(LogisticRegressionPytorch, self).__init__()\n",
    "        # LAYER DEFINITION MISSING\n",
    "    def forward(self, x):\n",
    "        # RETURN VALUE MISSING\n",
    "        \n",
    "logreg_pytorch = LogisticRegressionPytorch(d, m)\n",
    "print(logreg_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c9b500",
   "metadata": {},
   "source": [
    "### Model training and evaluation\n",
    "First we have to define a loss function. *Double check that output of the network and the expected input of the loss function match!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d71d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(logreg_pytorch.parameters(), lr=0.01)\n",
    "# DEFINITION OF LOSS FUNCTION MISSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_epochs = 10000  # Number of training steps\n",
    "X_train_T = torch.Tensor(X_train)  # Automatically casts to foat\n",
    "y_train_T = torch.from_numpy(y_train)  # Does not cast to float\n",
    "for epoch in range(no_epochs):  # Loop over the dataset multiple times\n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward + backward + optimize\n",
    "    outputs = logreg_pytorch(X_train_T)\n",
    "    # SOMETHING'S MISSING HERE\n",
    "    # SOMETHING'S MISSING HERE\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4ac966",
   "metadata": {},
   "source": [
    "Now the trained model is evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547c93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = logreg_pytorch(X_train_T)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "train_correct = (predicted == y_train_T).sum().item()\n",
    "\n",
    "X_test_T = torch.Tensor(X_test)  # Automatically casts to float\n",
    "y_test_T = torch.from_numpy(y_test)  # Does not cast to float\n",
    "outputs = logreg_pytorch(X_test_T)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "test_correct = (predicted == y_test_T).sum().item()\n",
    "\n",
    "print(\"Training error:\", train_correct/train_n, \"Test error:\", test_correct/test_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f17b1",
   "metadata": {},
   "source": [
    "### Model visualization\n",
    "For our visualization, we need the parameters of the model in the followoing format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a72429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Weights:\", ws, \"biases:\", bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f103742",
   "metadata": {},
   "source": [
    "You can get a named list of all parameters of your Pytorch model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5110dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in logreg_pytorch.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24302024",
   "metadata": {},
   "source": [
    "Now get the parameters as in the format as above.\n",
    "To convert the trainable tensors to nupy arrays you may want to use `.detach().numpy()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe89f3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_torch = logreg_pytorch.fc.weight.detach().numpy()\n",
    "# SOMETHING MISSING HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83257868",
   "metadata": {},
   "source": [
    "Let's do the plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ef301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot\n",
    "plt.figure(2)\n",
    "ax = plt.gca()\n",
    "\n",
    "# Classify each point on the grid\n",
    "X_Z_T = torch.Tensor(np.c_[xx.ravel(), yy.ravel()])  # Automatically casts to float\n",
    "outputs = logreg_pytorch(X_Z_T)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "Z = predicted.numpy().reshape(xx.shape)\n",
    "\n",
    "# Plot points\n",
    "ax.scatter(X[:,0], X[:,1], s=45, c=class_colours, edgecolor=plt.cm.gray(.95), lw=0.5, zorder=100)\n",
    "\n",
    "# Plot classifications\n",
    "cax = ax.matshow(Z, cmap=cmap, origin=\"lower\", extent=[xl, xh, yl, yh], aspect=\"auto\", alpha=.4)       \n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.grid(False)\n",
    "\n",
    "for i in np.arange(m-1):\n",
    "    for j in np.arange(i+1,m):\n",
    "        w = ws_torch[i]-ws_torch[j]\n",
    "        b = bs_torch[i]-bs_torch[j]\n",
    "        b = -b/w[1]\n",
    "        a = -w[0]/w[1]\n",
    "        x_line = np.arange(xl, xh, res)\n",
    "        y_line = a * x_line + b \n",
    "        ax.plot(x_line, y_line, \"k\", lw=1, ls='--')\n",
    "\n",
    "plt.xlim(xl, xh)\n",
    "plt.ylim(yl, yh)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa729e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
